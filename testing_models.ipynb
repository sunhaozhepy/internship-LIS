{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We use ABX tests to test the discriminative ability of our models. We use h5features and ABXpy modules of cognitive machine learning (CoML) research team.\n",
        "\n",
        "https://cognitive-ml.fr/\n",
        "\n",
        "https://github.com/bootphon"
      ],
      "metadata": {
        "id": "cEJlYnuqsGa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir abx\n",
        "%cd abx\n",
        "!git clone https://github.com/bootphon/h5features.git\n",
        "%cd h5features\n",
        "!python setup.py build && python setup.py install\n",
        "!pytest -v ./test"
      ],
      "metadata": {
        "id": "Q0lxd_mPsLtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../..\n",
        "!rm -rf abx\n",
        "!git clone https://github.com/bootphon/ABXpy.git\n",
        "%cd ABXpy"
      ],
      "metadata": {
        "id": "HuvUQ0kYsNwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout 7b254b99d6ce3f386f45d3da9c92cd45720dd9dd\n",
        "!module load gcc/4.7.2 \n",
        "!make install\n",
        "!make test\n",
        "%cd ..\n",
        "!rm -rf ABXpy"
      ],
      "metadata": {
        "id": "N7PbURJnsQTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Generating test stimuli\n",
        "\n",
        "We use text-to-speech (TTS) systems to automatically generate sound stimuli for a given language. We choose MBROLA voices, which has 29 usable languages in total (for details of \"usable\" languages see the corresponding training .ipynb file), one or several voices for each language, and we use espeak-ng as its front-end.\n",
        "\n",
        "In the first stage of this project, we aim to generate every possible syllable of each language. We proceed like this:\n",
        "\n",
        "1. Install espeak-ng, MBROLA system and MBROLA voices;\n",
        "\n",
        "2. Go to [LAPSyD DATABASE](https://lapsyd.huma-num.fr/lapsyd/index.php) to get all the vowels and consonants of a language;\n",
        "\n",
        "3. Find a nonce word generator (ike [this one](http://akana.conlang.org/tools/awkwords/)) and generate all possible syllables based on a CV rule (consonant+vowel). To do this, set a very big number of samples and filter all duplicate diphones.\n",
        "\n",
        "4. As the espeak-ng can only speak from X-SAMPA phonemes, we need to convert our generated IPA phonemes to X-SAMPA format. We use [this one](https://tools.lgm.cl/xsampa.html) but do make tests in espeak-ng to test the quality of the conversions.\n",
        "\n",
        "5. (see the code cell below) We now have a set of diphones seperated by blankspaces:\n",
        "\n",
        "  1. We split them into separate diphones;\n",
        "\n",
        "  2. We wrap these diphones in \"[[...]]\" to tell espeak-ng that the item is a set of phonemes; \n",
        "\n",
        "  3. We use espeak-ng to speak them and store the result in xxx.wav files where xxx is the diphone."
      ],
      "metadata": {
        "id": "bSDY-SzZHVMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install espeak-ng, MBROLA and MBROLA voices\n",
        "!apt-get install espeak-ng mbrola\n",
        "!git clone https://github.com/numediart/MBROLA-voices.git\n",
        "!mkdir /usr/share/mbrola\n",
        "!mv ./MBROLA-voices/data/* /usr/share/mbrola/"
      ],
      "metadata": {
        "id": "G2Ae422MU8B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paste your generated X-SAMPA phonemes here\n",
        "phonemes = \"\"\n",
        "phonemes = phonemes.split(' ')\n",
        "\n",
        "phonemes_processed = ['\"[[' + phonemes[i] + ']]\"' for i in range(len(phonemes))]\n",
        "\n",
        "for i in range(len(phonemes)):\n",
        "  phoneme = phonemes[i]\n",
        "  phoneme_processed = phonemes_processed[i]\n",
        "  !espeak-ng -v mb-en1 -w {phoneme}.wav {phoneme_processed}"
      ],
      "metadata": {
        "id": "rb0hCtwHWxfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MBROLA: https://github.com/numediart/MBROLA\n",
        "\n",
        "MBROLA voices: https://github.com/numediart/MBROLA-voices\n",
        "\n",
        "espeak-ng: https://github.com/espeak-ng/espeak-ng"
      ],
      "metadata": {
        "id": "630Asg2CU8c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Processing test stimuli\n",
        "In the last section we have generated a bunch of .wav files. Now we want to do two things: we summarize their information in an item file, and we convert them into numpy arrays and extract mel-spectrogram features from them."
      ],
      "metadata": {
        "id": "VVz8THyuFv04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# item file generation\n",
        "with open(\"stimuli.item\", 'w') as f:\n",
        "  pass"
      ],
      "metadata": {
        "id": "QbPw8c6NtNXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction and storage in HDF5 format\n",
        "import librosa\n",
        "import os\n",
        "import h5features as h5f\n",
        "\n",
        "id_list = []\n",
        "feature_list = []\n",
        "\n",
        "for f in os.listdir():\n",
        "  if f.endswith(\".wav\"):\n",
        "    wav_file = f\n",
        "\n",
        "    # we can also try sr, y = scipy.io.wavfile.read(wav_file) and see which one is more efficient (test if normalizing affects the final mel-spectrogram)\n",
        "    y, sr = librosa.load(wav_file)\n",
        "\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "\n",
        "    feature_list.append(mel_spectrogram)\n",
        "\n",
        "    id = f.replace(\".wav\", '')\n",
        "    id_list.append(id)\n",
        "\n",
        "time_list = [] # .wav file has a sample rate of 44.1kHz, the default window length for librosa is 2048, so it gives a \n",
        "\n",
        "feature_file = \"stimuli.features\"\n",
        "\n",
        "with h5f.Writer(feature_file) as writer:   \n",
        "  data = h5f.Data(utts=id_list, times=time_list, feats=feature_list, check=True)\n",
        "  writer.write(data, 'features')"
      ],
      "metadata": {
        "id": "bRkmxeqBc-dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_task.py stimuli.item abx.task"
      ],
      "metadata": {
        "id": "QYtqrQ_w0HoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_abx.py stimuli.features abx.task res_folder res_id cos true"
      ],
      "metadata": {
        "id": "Xy3GnV8K0MBY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}