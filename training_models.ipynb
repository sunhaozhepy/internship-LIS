{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Computational modeling of infantsâ€™ early phonetic learning across languages\n",
        "\n",
        "This is the workspace for Haozhe Sun's intern project, during the period of 2022-06-06 to 2022-07-31, at the QARMA research group of the LIS laboratory, in Marseille, France."
      ],
      "metadata": {
        "id": "yhoeXyNQAcTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube\n",
        "!pip install pydub\n",
        "!pip install googletrans==3.1.0a0\n",
        "!pip install datasets\n",
        "!pip install ujson"
      ],
      "metadata": {
        "id": "MJ7J1khv7TNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Searching via YouTube Data API\n",
        "\n",
        "We scrape stories for children on YouTube in different languages to get enough training data.\n",
        "\n",
        "Inspired from https://github.com/youtube/api-samples/blob/master/python/search.py\n",
        "\n",
        "API: https://cloud.google.com/console\n",
        "\n",
        "Relevant API doc: https://developers.google.com/youtube/v3/docs/search/list\n",
        "\n",
        "**WARNING: the developer key should never be pushed to github.**"
      ],
      "metadata": {
        "id": "mgZtqvNZ1PUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "def get_api():\n",
        "  DEVELOPER_KEY = ''\n",
        "  YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "  YOUTUBE_API_VERSION = 'v3'\n",
        "  youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,\n",
        "                  developerKey=DEVELOPER_KEY)\n",
        "  return youtube\n",
        "\n",
        "def search(youtube, **kwargs):\n",
        "  search_response = youtube.search().list(**kwargs).execute()\n",
        "  return search_response"
      ],
      "metadata": {
        "id": "x7cA5CJ51KiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ujson as json\n",
        "from googleapiclient.errors import HttpError\n",
        "from googletrans import Translator\n",
        "\n",
        "nb_pages_per_lang = 3 # the daily quota is 100 search pages, that means ~3 pages per language = ~150 items per language = ~ 10-50 hours per language before filtering\n",
        "\n",
        "maxResults = 50  # max allowed by page\n",
        "part = 'id, snippet'\n",
        "item_type = 'video'\n",
        "duration = \"medium\"\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "# ISO-639-1 code, see http://www.loc.gov/standards/iso639-2/php/code_list.php\n",
        "lang_ids = {\n",
        "    \"afrikaans\": \"af\", \n",
        "    \"arabic\": \"ar\", \n",
        "    \"chinese\": \"zh-CN\", # googletrans' convention for detecting language\n",
        "    \"croatian\": \"hr\", \n",
        "    \"czech\": \"cs\", \n",
        "    \"german\": \"de\", \n",
        "    \"estonian\": \"et\", \n",
        "    \"english\": \"en\", \n",
        "    \"spanish\": \"es\", \n",
        "    \"french\": \"fr\", \n",
        "    \"greek\": \"el\", \n",
        "    \"hebrew\": \"iw\", # the new standard is \"he\", acceptable input, but not the output convention of googletrans\n",
        "    \"korean\": \"ko\", \n",
        "    \"hungarian\": \"hu\", \n",
        "    \"icelandic\": \"is\",\n",
        "    \"indonesian\": \"id\", \n",
        "    \"hindi\": \"hi\", \n",
        "    \"italian\": \"it\", \n",
        "    \"japanese\": \"ja\", \n",
        "    \"lithuanian\": \"lt\", \n",
        "    \"malay\": \"ms\", \n",
        "    \"dutch\": \"nl\", \n",
        "    \"maori\": \"mi\", \n",
        "    \"polish\": \"pl\", \n",
        "    \"portuguese\": \"pt\", \n",
        "    \"romanian\": \"ro\", \n",
        "    \"swedish\": \"sv\", \n",
        "    \"telugu\": \"te\", \n",
        "    \"turkish\": \"tr\"\n",
        "}\n",
        "          \n",
        "# languages names in English to avoid Unicode complications\n",
        "# available MBROLA voices without Iranian as it doesn't have a ISO-639-1 code, and breton which is not supported by googletrans, and latin which has no query result on YouTube\n",
        "# in total 29 languages\n",
        "# no difference in dialects as they won't matter in a grapheme-based video query\n",
        "langs = [\"afrikaans\", \"arabic\", \"chinese\", \"croatian\", \"czech\", \"german\", \"estonian\", \"english\", \"spanish\", \"french\", \"greek\", \"hebrew\", \"korean\", \"hungarian\", \"icelandic\",\n",
        "    \"indonesian\", \"hindi\", \"italian\", \"japanese\", \"lithuanian\", \"malay\", \"dutch\", \"maori\", \"polish\", \"portuguese\", \"romanian\", \"swedish\", \"telugu\", \"turkish\"]\n",
        "lang_qs = {lang: translator.translate('stories for children', dest=lang_ids[lang]).text for lang in langs}\n",
        "\n",
        "youtube = get_api()\n",
        "res = {lang: [] for lang in langs}\n",
        "\n",
        "for lang in langs:\n",
        "  for page in range(nb_pages_per_lang):\n",
        "    try:\n",
        "      if page == 0:\n",
        "        res[lang].append(search(youtube, maxResults=maxResults, part=part, type=item_type, q=lang_qs[lang], videoDuration=duration))\n",
        "      else:\n",
        "        pageToken = res[lang][-1]['nextPageToken']\n",
        "        res[lang].append(search(youtube, pageToken=pageToken, maxResults=maxResults, part=part, type=item_type, q=lang_qs[lang], videoDuration=duration))\n",
        "    except HttpError as e:\n",
        "      print(f'An HTTP error occurred for {lang}: {e.resp.status}: {e.content}')\n",
        "\n",
        "with open(\"results.json\", 'w') as f:\n",
        "  json.dump(res, f)"
      ],
      "metadata": {
        "id": "ghOssduV4DBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Constructing training dataset"
      ],
      "metadata": {
        "id": "7fLQlsVEl9xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from result.json to videos.json\n",
        "# we further filter search results by inspecting the language of title and description\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "with open(\"results.json\", 'r') as f:\n",
        "  res = json.load(f)\n",
        "\n",
        "if not os.path.exists(\"./videos\"):\n",
        "  os.mkdir(\"./videos\")\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "for lang in tqdm(langs):\n",
        "  video_dict = {lang: []}\n",
        "  for search in res[lang]:\n",
        "    for item in search[\"items\"]:\n",
        "      if translator.detect(item[\"snippet\"][\"title\"]).lang == lang_ids[lang] and translator.detect(item[\"snippet\"][\"description\"]).lang == lang_ids[lang]:\n",
        "        video_dict[lang].append(item[\"id\"][\"videoId\"])\n",
        "\n",
        "  with open(f\"./videos/videos_{lang}.json\", 'w') as g: # if you are on colab, download and conserve these files! (or download the results.json file if you like)\n",
        "    json.dump(video_dict, g)"
      ],
      "metadata": {
        "id": "x8fKBh4gb5v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We construct the dataset by feature extraction: we use librosa to automatically calculate mel spectrograms as input features.\n",
        "\n",
        "In the first part of this project we ignore all ordering/positional information in a given audio file and across audio files. As a result, we use all feature vectors of every window, as a whole bag of vectors, as the input data to the model; for this process we use HuggingFace datasets and do batch mapping on them."
      ],
      "metadata": {
        "id": "WsqGKh_xjdW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "from moviepy.editor import *\n",
        "import pydub\n",
        "import numpy as np\n",
        "import os\n",
        "import ujson as json\n",
        "\n",
        "# the speed of video processing: ~80 videos per hour\n",
        "\n",
        "# inspired from https://stackoverflow.com/questions/53633177/how-to-read-a-mp3-audio-file-into-a-numpy-array-save-a-numpy-array-to-mp3\n",
        "# we can also try y, sr = librosa.load(mp3_file) and see which one is more efficient (test if normalizing affects the final mel-spectrogram)\n",
        "def mp3tonumpy(f):\n",
        "  a = pydub.AudioSegment.from_mp3(f).set_channels(1)\n",
        "  y = np.array(a.get_array_of_samples())\n",
        "  return np.float32(y) / 2**15\n",
        "\n",
        "with open(\"./videos/videos_afrikaans.json\", 'r') as h: # here we have split the videos.json file to prevent the disk from being occupied entirely\n",
        "  video_dict = json.load(h)\n",
        "\n",
        "for lang in video_dict:\n",
        "  empty_dict = {\n",
        "      \"data\": []\n",
        "  }\n",
        "  count = 0\n",
        "  for video in video_dict[lang]:\n",
        "    # search for a certain youtube video and download it\n",
        "    video = f\"https://www.youtube.com/watch?v={video}\"\n",
        "    obj = YouTube(video)\n",
        "    obj.streams.filter(only_audio=True, file_extension=\"mp4\").order_by('abr').desc().last().download()\n",
        "\n",
        "    mp4_file = None\n",
        "    mp3_file = None\n",
        "\n",
        "    for f in os.listdir():\n",
        "      if f.endswith(\".mp4\"):\n",
        "        mp4_file = f\n",
        "        mp3_file = f.replace(\".mp4\", \".mp3\")\n",
        "\n",
        "    # convert .mp4 to .mp3\n",
        "    audioclip = AudioFileClip(mp4_file)\n",
        "    audioclip.write_audiofile(mp3_file)\n",
        "    audioclip.close()\n",
        "\n",
        "    # remove the original .mp4 file\n",
        "    os.remove(mp4_file)\n",
        "\n",
        "    # convert .mp3 file to numpy array\n",
        "    x = mp3tonumpy(mp3_file)\n",
        "\n",
        "    # we can't save a big numpy array as one element of csv file, as it will be treated as a string and all center elements will be replaced by '...'\n",
        "    # even if we try to store them as a row, it's difficult to handle data with different length when HuggingFace reads csv files\n",
        "    # so we use json file to store our numpy arrays (we still need to convert them to a list as json can't encode numpy arrays)\n",
        "    data = {}\n",
        "    data[\"audio\"] = x.tolist()\n",
        "    empty_dict[\"data\"].append(data)\n",
        "\n",
        "    os.remove(mp3_file)\n",
        "    if not os.path.exists(\"./data\"):\n",
        "      os.mkdir(\"./data\")\n",
        "\n",
        "    # save the file every 5 videos\n",
        "    count += 1\n",
        "    if count % 5 == 0:\n",
        "      with open(f\"./data/{lang}_{count // 5 - 1}.json\", 'w') as f:\n",
        "        json.dump(empty_dict, f)\n",
        "      index = count // 5 - 1\n",
        "      !gzip ./data/{lang}_{index}.json\n",
        "      empty_dict = {\n",
        "        \"data\": []\n",
        "    }\n",
        "  if empty_dict[\"data\"] != []:\n",
        "    with open(f\"./data/{lang}_{count // 5}.json\", 'w') as f:\n",
        "      json.dump(empty_dict, f)\n",
        "    index = count // 5\n",
        "    !gzip ./data/{lang}_{index}.json"
      ],
      "metadata": {
        "id": "HuW3vGXUPPyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on colab, we upload to HuggingFace Hub in order not to lose progress\n",
        "from huggingface_hub import notebook_login, create_repo, Repository\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "sgesVijb_2Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo_url = create_repo(name=\"youtube-multilingual-child-stories-44.1kHz\", repo_type=\"dataset\")\n",
        "repo = Repository(local_dir=\"youtube-multilingual-child-stories-44.1kHz\", clone_from=repo_url)\n",
        "!cp ./data/ youtube-multilingual-child-stories-44.1kHz/\n",
        "repo.push_to_hub(private=True)"
      ],
      "metadata": {
        "id": "toJH1ZGd02_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "qux-_JRFO-CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def transform(examples):\n",
        "  processed_data = np.zeros((0, 128))\n",
        "  for data in examples[\"audio\"]:\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=np.asarray(data), sr=44100, n_mels=128)\n",
        "    processed_data = np.concatenate((processed_data, mel_spectrogram.T), axis=0)\n",
        "  return {\"data\": processed_data}"
      ],
      "metadata": {
        "id": "5FyIJD1VDhVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"sunhaozhepy/youtube-multilingual-child-stories-44.1kHz\", data_files=\"data/afrikaans_0.json.gz\", field=\"data\", split=\"train\") # on colab do this\n",
        "# we can use data_files=\"data/afrikaans_*.json.gz\" to load every json.gz file\n",
        "# raw_dataset = load_dataset(\"json\", data_files=\"data/afrikaans_0.json.gz\", field=\"data\", split=\"train\") # locally do this"
      ],
      "metadata": {
        "id": "aQNYYHclEH0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = raw_dataset.map(transform, remove_columns=raw_dataset.column_names, batched=True, batch_size=64)\n",
        "dataset.set_format(\"torch\")\n",
        "print(len(dataset))\n",
        "\n",
        "split_dataset = dataset.train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "Wydv5cTiQjeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(split_dataset[\"train\"], batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(split_dataset[\"test\"], batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "q_wq8wAwEMPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training models for different languages\n",
        "\n",
        "This task being unsupervised representation learning, we implement three popular models of this kind: PCA, linear Autoencoder, and deep Autoencoder."
      ],
      "metadata": {
        "id": "9PGFHxEq5v_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "qdpHFzmthY4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 PCA\n",
        "\n",
        "Note that normally PCA is fit in an offline, matrix-factorization-based way, but to scale things up, we have to think of a way to do online training. That is to say, we sample a mini-batch of data in our dataset, and do gradient descent (SGD) on this batch of data. This problem is known in literature as streaming PCA or streaming k-PCA where k stands for the k leading eigenvectors of the data covariance matrix.\n",
        "\n",
        "To do so, we use Oja's algorithm (see the paper for details)."
      ],
      "metadata": {
        "id": "JZG1L3Sskj2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Linear Autoencoder\n",
        "\n",
        "Autoencoders are artificial neural networks that consist of an encoder and a decoder: the encoder projects the input to a feature space, and the decoder projects the encoding back to the input space with the least reconstruction error possible.\n",
        "\n",
        "To compare with PCA, we first implement a linear autoencoder with one hidden layer, which is equivalent to a PCA model with no orthogonality/norm constraint."
      ],
      "metadata": {
        "id": "kb77fQbik1l4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearAutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Linear(128, 8)\n",
        "    self.decoder = nn.Linear(8, 128)\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoding = self.encoder(x)\n",
        "    reconstructed_x = self.decoder(encoding)\n",
        "    return encoding, reconstructed_x"
      ],
      "metadata": {
        "id": "NbP3Zhqg5vqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Deep Autoencoder\n",
        "\n",
        "SImilar as above, we apply a full autoencoder with a deep encoder-decoder architecture, and with activation layers."
      ],
      "metadata": {
        "id": "9Dpd8iiolDTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16, 8)\n",
        "    )\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(8, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 128),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoding = self.encoder(x)\n",
        "    reconstructed_x = self.decoder(encoding)\n",
        "    return encoding, reconstructed_x"
      ],
      "metadata": {
        "id": "VatmzwLR6Sl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using {device}.\")\n",
        "\n",
        "model = AutoEncoder().to(device)\n",
        "  \n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "step = 0\n",
        "best_val_loss = float(\"inf\")\n",
        "train_loss = 0\n",
        "for batch in train_dataloader:\n",
        "  model.train()\n",
        "  batch = batch[\"data\"].to(device) \n",
        "  optimizer.zero_grad()\n",
        "  _, outputs = model(batch)\n",
        "  loss = loss_fn(outputs, batch)\n",
        "  train_loss += loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  step += 1\n",
        "\n",
        "  if step % 50 == 0:\n",
        "    print(f\"step {step}, training loss = {train_loss / 50}\")\n",
        "    train_loss = 0\n",
        "\n",
        "  if step % 500 == 0:\n",
        "    print(\"validation loop...\")\n",
        "    eval_loss = 0\n",
        "    model.eval()\n",
        "    for eval_batch in test_dataloader:\n",
        "      with torch.no_grad():\n",
        "        eval_batch = eval_batch[\"data\"].to(device) \n",
        "        _, outputs = model(eval_batch)\n",
        "        loss = loss_fn(outputs, eval_batch)\n",
        "        eval_loss += loss\n",
        "    eval_loss /= len(test_dataloader)\n",
        "    print(f\"validation loss = {eval_loss}\")\n",
        "    if eval_loss < best_val_loss:\n",
        "      print(\"Saving checkpoint!\")\n",
        "      best_val_loss = eval_loss\n",
        "      torch.save(model.state_dict(), f'/content/drive/My Drive/Colab Notebooks/checkpoints/autoencoder_afrikaans_best_file_0.pt') # model_language_\"best\"_dataset"
      ],
      "metadata": {
        "id": "jpt4m98d575X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clbxltRvtxsz"
      },
      "source": [
        "## Storing results and anonymization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOiDzOYbtxsz"
      },
      "source": [
        "Just keep a list of videos url on disk + some hash of the wavs (along with search params and timestamp for search). We can keep language id (general or couple with vad/diarization), vad, (anonym) speaker diarization...\n",
        "\n",
        "Publish the search procedure and code but do not keep any of the metadata and wav data (and caption).\n",
        "Rationale is to avoid any issue with people who would like some public info about them to be removed and scientifically it means:\n",
        "\n",
        "  1. we should check things work out with variants of training set\n",
        "  2. this is like the idea that you cannot test the same persons twice in experiments in practice, but that your conclusions should hold if someone reproduces the experiments with other people.\n",
        "  \n",
        "This means retraining all relevant models every-time you want to compare them if some video became unavailable or hash changed.\n",
        "\n",
        "One issue to keep in mind is that we don't want to use models where the training data could be reconstructed from the model parameters.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "9629ffed8c339fa6a6a976c8da87c367e5e4e97bb71e3fca914b301a97edf693"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
